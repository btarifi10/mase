{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity, get_logger\n",
    "\n",
    "from chop.passes.graph.analysis import (\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    ")\n",
    "from chop.passes.graph import (\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    ")\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.ir.graph.mase_graph import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "\n",
    "set_logging_verbosity(\"info\")\n",
    "\n",
    "logger = get_logger(\"chop\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "batch_size = 8\n",
    "model_name = \"jsc-tiny\"\n",
    "dataset_name = \"jsc\"\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = {\"x\": next(iter(data_module.train_dataloader()))[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from chop.passes.graph.utils import get_parent_name\n",
    "\n",
    "# define a new model\n",
    "class JSC_Three_Linear_Layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSC_Three_Linear_Layers, self).__init__()\n",
    "        self.seq_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),  # 0\n",
    "            nn.ReLU(16),  # 1\n",
    "            nn.Linear(16, 16),  # linear  2\n",
    "            nn.ReLU(16),  # 2\n",
    "            nn.Linear(16, 16),  # linear  3\n",
    "            nn.ReLU(16),  # 3\n",
    "            nn.Linear(16, 5),   # linear  4\n",
    "            nn.ReLU(5),  # 5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq_blocks(x)\n",
    "\n",
    "\n",
    "model = JSC_Three_Linear_Layers()\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.fx.graph.Graph at 0x7fe523e4c4d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg.fx_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})\n",
      "    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})\n",
      "    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})\n",
      "    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})\n",
      "    %seq_blocks_4 : [num_users=1] = call_module[target=seq_blocks.4](args = (%seq_blocks_3,), kwargs = {})\n",
      "    %seq_blocks_5 : [num_users=1] = call_module[target=seq_blocks.5](args = (%seq_blocks_4,), kwargs = {})\n",
      "    %seq_blocks_6 : [num_users=1] = call_module[target=seq_blocks.6](args = (%seq_blocks_5,), kwargs = {})\n",
      "    %seq_blocks_7 : [num_users=1] = call_module[target=seq_blocks.7](args = (%seq_blocks_6,), kwargs = {})\n",
      "    return seq_blocks_7\n",
      "Network overview:\n",
      "{'placeholder': 1, 'get_attr': 0, 'call_function': 0, 'call_method': 0, 'call_module': 8, 'output': 1}\n",
      "Layer types:\n",
      "[BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=16, out_features=32, bias=True), ReLU(inplace=True), Linear(in_features=32, out_features=64, bias=True), ReLU(inplace=True), Linear(in_features=64, out_features=5, bias=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "from chop.passes.graph.analysis.report.report_graph import report_graph_analysis_pass\n",
    "\n",
    "def instantiate_linear(in_features, out_features, bias):\n",
    "    if bias is not None:\n",
    "        bias = True\n",
    "    return nn.Linear(\n",
    "        in_features=in_features,\n",
    "        out_features=out_features,\n",
    "        bias=bias)\n",
    "\n",
    "def instantiate_relu(in_features):\n",
    "    return nn.ReLU(in_features)\n",
    "\n",
    "def instantiate_batchnorm(in_features):\n",
    "    return nn.BatchNorm1d(in_features)\n",
    "\n",
    "def redefine_linear_transform_pass(graph, pass_args=None):\n",
    "    main_config = pass_args.pop('config')\n",
    "    default = main_config.pop('default', None)\n",
    "    if default is None:\n",
    "        raise ValueError(f\"default value must be provided.\")\n",
    "    i = 0\n",
    "    for node in graph.fx_graph.nodes:\n",
    "        i += 1\n",
    "        # if node name is not matched, it won't be tracked\n",
    "        config = main_config.get(node.name, default)['config']\n",
    "        name = config.get(\"name\", None)\n",
    "\n",
    "        next_node = node.next\n",
    "        prev_node = node.prev\n",
    "        if name is not None:\n",
    "            ori_module = graph.modules[node.target]\n",
    "            if not isinstance(ori_module, nn.Linear):\n",
    "                raise ValueError(f\"Node {node.name} is not a linear layer.\")\n",
    "\n",
    "            in_features = ori_module.in_features\n",
    "            out_features = ori_module.out_features\n",
    "            bias = ori_module.bias\n",
    "            if name == \"output_only\" or name == \"both\":\n",
    "                output_multiplier = config.get(\"output_multiplier\", config.get(\"channel_multiplier\"))\n",
    "                if not output_multiplier:\n",
    "                    logger.warning(f\"Could not find output_multiplier or channel_multiplier for node {node.name}. Using value of 1.\")\n",
    "                    output_multiplier = 1\n",
    "                out_features = out_features * output_multiplier\n",
    "            elif name == \"input_only\" or name == \"both\":\n",
    "                input_multiplier = config.get(\"input_multiplier\", config[\"channel_multiplier\"])\n",
    "                if not input_multiplier:\n",
    "                    logger.warning(f\"Could not find input_multiplier or channel_multiplier for node {node.name}. Using value of 1.\")\n",
    "                    input_multiplier = 1\n",
    "                in_features = in_features * input_multiplier\n",
    "\n",
    "            # Find the previous linear module\n",
    "            # All the previous modules should be either Linear, ReLU, or BatchNorm1d\n",
    "            # The batchnorm1d and relu layers should be resized to the new in_features\n",
    "            # The previous linear layer's output should be scaled to match the new in_features\n",
    "            if name == \"input_only\" or name == \"both\":\n",
    "                valid = False\n",
    "                prev_node = node.prev\n",
    "                prev_module = graph.modules.get(prev_node.target, None)\n",
    "                while (prev_node and prev_module and not valid):\n",
    "                    if isinstance(prev_module, nn.Linear):\n",
    "                        valid = True\n",
    "                    prev_node = prev_node.prev\n",
    "                    prev_module = graph.modules.get(prev_node.target, None)\n",
    "                \n",
    "                if valid:\n",
    "                    prev_node = node.prev\n",
    "                    prev_module = graph.modules[prev_node.target]\n",
    "                    while (not isinstance(prev_module, nn.Linear)):\n",
    "                        if isinstance(prev_module, nn.ReLU):\n",
    "                            new_prev_module = instantiate_relu(in_features)\n",
    "                            parent_name, name = get_parent_name(prev_node.target)\n",
    "                            setattr(graph.modules[parent_name], name, new_prev_module)\n",
    "                        elif isinstance(prev_module, nn.BatchNorm1d):\n",
    "                            new_prev_module = instantiate_batchnorm(in_features)\n",
    "                            parent_name, name = get_parent_name(prev_node.target)\n",
    "                            setattr(graph.modules[parent_name], name, new_prev_module)\n",
    "                        prev_node = prev_node.prev\n",
    "                        prev_module = graph.modules[prev_node.target]\n",
    "                    assert isinstance(prev_module, nn.Linear)\n",
    "                    new_prev_module = instantiate_linear(prev_module.in_features, in_features, prev_module.bias)\n",
    "                    parent_name, name = get_parent_name(prev_node.target)\n",
    "                    setattr(graph.modules[parent_name], name, new_prev_module)\n",
    "                else:\n",
    "                    logger.warning(f\"Node {node.name} is not connected to a linear layer on the input side. \" + \n",
    "                                   \"Skipping input transformation.\")\n",
    "                    in_features = ori_module.in_features\n",
    "\n",
    "            if name == \"output_only\" or name == \"both\":\n",
    "                valid = False\n",
    "                next_node = node.next\n",
    "                next_module = graph.modules.get(next_node.target, None)\n",
    "                while (next_node and not valid):\n",
    "                    if isinstance(next_module, nn.Linear):\n",
    "                        valid = True\n",
    "                    next_node = next_node.prev\n",
    "                    next_module = graph.modules.get(next_node.target, None)\n",
    "\n",
    "\n",
    "                if valid:\n",
    "                    next_node = node.next\n",
    "                    next_module = graph.modules[next_node.target]\n",
    "                    while (not isinstance(next_module, nn.Linear)):\n",
    "                        if isinstance(next_module, nn.ReLU):\n",
    "                            new_next_module = instantiate_relu(out_features)\n",
    "                            parent_name, name = get_parent_name(next_node.target)\n",
    "                            setattr(graph.modules[parent_name], name, new_next_module)\n",
    "                        elif isinstance(next_module, nn.BatchNorm1d):\n",
    "                            new_next_module = instantiate_batchnorm(out_features)\n",
    "                            parent_name, name = get_parent_name(next_node.target)\n",
    "                            setattr(graph.modules[parent_name], name, new_next_module)\n",
    "                        next_node = next_node.next\n",
    "                        next_module = graph.modules[next_node.target]\n",
    "                    assert isinstance(next_module, nn.Linear)\n",
    "                    new_next_module = instantiate_linear(out_features, next_module.out_features, next_module.bias)\n",
    "                    parent_name, name = get_parent_name(next_node.target)\n",
    "                    setattr(graph.modules[parent_name], name, new_next_module)\n",
    "                else:\n",
    "                    logger.warning(f\"Node {node.name} is not connected to a linear layer on the output side.\" + \n",
    "                                   \"Skipping output transformation.\")\n",
    "                    out_features = ori_module.out_features\n",
    "\n",
    "             # Finally, set the new linear module.\n",
    "            new_module = instantiate_linear(in_features, out_features, bias)\n",
    "            parent_name, name = get_parent_name(node.target)\n",
    "            setattr(graph.modules[parent_name], name, new_module)\n",
    "\n",
    "    return graph, {}\n",
    "\n",
    "\n",
    "pass_config = {\n",
    "    \"by\": \"name\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "    \"seq_blocks_2\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"output_only\",\n",
    "            \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_4\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"both\",\n",
    "            \"input_multiplier\": 2,\n",
    "            \"output_multiplier\": 4,\n",
    "        }\n",
    "    },\n",
    "    \"seq_blocks_6\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"input_only\",\n",
    "            \"channel_multiplier\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# this performs the architecture transformation based on the config\n",
    "mg, _ = redefine_linear_transform_pass(graph=mg, pass_args={\"config\": pass_config})\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "mg, _ = report_graph_analysis_pass(mg, {})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "\n",
    "from chop.passes.graph import (\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    ")\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.ir.graph.mase_graph import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "\n",
    "set_logging_verbosity(\"info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "model_name = \"vgg7\"\n",
    "dataset_name = \"cifar10\"\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "dataset_info = get_dataset_info(dataset_name)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "model_oneshot = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "model_lt = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "dummy_in = {\"x\": next(iter(data_module.train_dataloader()))[0]}\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg_base = MaseGraph(model=model_base)\n",
    "mg_oneshot = MaseGraph(model=model_oneshot)\n",
    "mg_lt = MaseGraph(model=model_lt)\n",
    "\n",
    "mg_base, _ = init_metadata_analysis_pass(mg_base, None)\n",
    "mg_base, _ = add_common_metadata_analysis_pass(mg_base, {\"dummy_in\": dummy_in})\n",
    "mg_base, _ = add_software_metadata_analysis_pass(mg_base, None)\n",
    "\n",
    "mg_oneshot, _ = init_metadata_analysis_pass(mg_oneshot, None)\n",
    "mg_oneshot, _ = add_common_metadata_analysis_pass(mg_oneshot, {\"dummy_in\": dummy_in})\n",
    "mg_oneshot, _ = add_software_metadata_analysis_pass(mg_oneshot, None)\n",
    "\n",
    "mg_lt, _ = init_metadata_analysis_pass(mg_lt, None)\n",
    "mg_lt, _ = add_common_metadata_analysis_pass(mg_lt, {\"dummy_in\": dummy_in})\n",
    "mg_lt, _ = add_software_metadata_analysis_pass(mg_lt, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the VGG network and then pruning it to see final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_params = {\n",
    "    \"model_info\": model_info,\n",
    "    \"data_module\": data_module,\n",
    "    \"dataset_info\": dataset_info,\n",
    "    \"task\": \"cls\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0,\n",
    "    \"plt_trainer_args\": {\n",
    "        \"max_epochs\": 5,\n",
    "    }, \n",
    "    \"auto_requeue\": False,\n",
    "    \"save_path\": None,\n",
    "    \"visualizer\": None,\n",
    "    \"load_name\": None,\n",
    "    \"load_type\": None\n",
    "}\n",
    "\n",
    "base_train_params = default_train_params.copy()\n",
    "base_train_params[\"model\"] = mg_base.model\n",
    "base_train_params[\"plt_trainer_args\"][\"max_epochs\"] = 5\n",
    "\n",
    "oneshot_train_params_1 = default_train_params.copy()\n",
    "oneshot_train_params_1[\"model\"] = mg_oneshot.model\n",
    "oneshot_train_params_1[\"plt_trainer_args\"][\"max_epochs\"] = 3\n",
    "\n",
    "oneshot_train_params_2 = default_train_params.copy()\n",
    "oneshot_train_params_2[\"model\"] = mg_oneshot.model\n",
    "oneshot_train_params_2[\"plt_trainer_args\"][\"max_epochs\"] = 2\n",
    "\n",
    "lt_train_params = default_train_params.copy()\n",
    "lt_train_params[\"model\"] = mg_lt.model\n",
    "lt_train_params[\"plt_trainer_args\"][\"max_epochs\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 14.0 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | acc_val   | MulticlassAccuracy | 0     \n",
      "4 | acc_test  | MulticlassAccuracy | 0     \n",
      "5 | loss_val  | MeanMetric         | 0     \n",
      "6 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "56.118    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca9673ef8bf4f80a3a9575f5c8a7602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkt123/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/home/bkt123/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ebaa051b3d4eb58b2ed8146091dc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4bd27c68254ca7a3662b11a8a00375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkt123/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d91f86c67954c4a9551281cd6299c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch          0.289900004863739\n",
      "     test_loss_epoch         1.84405517578125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "from chop.actions import test, train\n",
    "\n",
    "train(**base_train_params)\n",
    "test(**base_train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Model after pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a047d5650f0f48d0b64c9a9e573dafa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.22779999673366547\n",
      "     test_loss_epoch        2.0580010414123535\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "from chop.passes.graph.transforms import (\n",
    "    prune_transform_pass,\n",
    ")\n",
    "\n",
    "pass_args = {\n",
    "    \"weight\":{\n",
    "        \"scope\" : \"global\",\n",
    "        \"granularity\" : \"elementwise\",\n",
    "        \"method\" :  \"l1-norm\",\n",
    "        \"sparsity\" : 0.7,},\n",
    "    \"activation\":{\n",
    "        \"scope\" : \"global\",\n",
    "        \"granularity\" : \"elementwise\",\n",
    "        \"method\" : \"l1-norm\",\n",
    "        \"sparsity\" : 0.7,\n",
    "    },\n",
    "}\n",
    " \n",
    "mg_base, _ = prune_transform_pass(mg_base, pass_args)\n",
    "\n",
    "base_train_params[\"model\"] = mg_base.model\n",
    "test(**base_train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One shot pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 14.0 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | acc_val   | MulticlassAccuracy | 0     \n",
      "4 | acc_test  | MulticlassAccuracy | 0     \n",
      "5 | loss_val  | MeanMetric         | 0     \n",
      "6 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "56.118    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c51ff0d1164d2c8e5e5be95a122598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f9f559e953487a94b081dc7d1050cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aa25b696954c14af6973f2066fe8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 14.0 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | acc_val   | MulticlassAccuracy | 0     \n",
      "4 | acc_test  | MulticlassAccuracy | 0     \n",
      "5 | loss_val  | MeanMetric         | 0     \n",
      "6 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "56.118    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d14487cdc34232b0c1a0fc12ffc6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d54d0edfd7e45059931557e21e5a860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e947cf5c6fe443a86365830d94cbcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f799e1e3a5747678978fa208f735ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.46970000863075256\n",
      "     test_loss_epoch        1.4413295984268188\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "train(**oneshot_train_params_1)\n",
    "\n",
    "mg_oneshot, _ = prune_transform_pass(mg_oneshot, pass_args)\n",
    "\n",
    "oneshot_train_params_2[\"model\"] = mg_oneshot.model\n",
    "train(**oneshot_train_params_2)\n",
    "test(**oneshot_train_params_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lottery Ticket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.passes.graph.utils import get_mase_op\n",
    "import copy, torch\n",
    "\n",
    "\n",
    "def prune_lottery_ticket(mg, pass_args):\n",
    "    overall_sparsity = pass_args[\"lottery_ticket\"][\"sparsity\"]\n",
    "    num_iterations = pass_args[\"lottery_ticket\"][\"num_iterations\"]\n",
    "\n",
    "    iteration_sparsity = 1 - ((1 - overall_sparsity) ** (1 / num_iterations))\n",
    "\n",
    "    data_module = MaseDataModule(\n",
    "        name=pass_args[\"config\"][\"dataset\"],\n",
    "        batch_size=pass_args[\"config\"][\"batch_size\"],\n",
    "        model_name=pass_args[\"config\"][\"model\"],\n",
    "        num_workers=0,\n",
    "    )\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup()\n",
    "\n",
    "    train_test_args = {\n",
    "        \"model\": mg.model,\n",
    "        \"model_info\": get_model_info(pass_args[\"config\"][\"model\"]),\n",
    "        \"data_module\": data_module,\n",
    "        \"dataset_info\": get_dataset_info(pass_args[\"config\"][\"dataset\"]),\n",
    "        \"task\": pass_args[\"config\"][\"task\"],\n",
    "        \"optimizer\": pass_args[\"config\"][\"optimizer\"],\n",
    "        \"learning_rate\": pass_args[\"config\"][\"learning_rate\"],\n",
    "        \"weight_decay\": pass_args[\"config\"][\"weight_decay\"],\n",
    "        \"plt_trainer_args\": {\n",
    "            \"max_epochs\": pass_args[\"config\"][\"max_epochs\"],\n",
    "        },\n",
    "        \"auto_requeue\": False,\n",
    "        \"save_path\": None,\n",
    "        \"visualizer\": None,\n",
    "        \"load_name\": None,\n",
    "        \"load_type\": None,\n",
    "    }\n",
    "\n",
    "    prune_args = {\n",
    "        \"weight\": {\n",
    "            \"scope\": pass_args[\"lottery_ticket\"][\"scope\"],\n",
    "            \"granularity\": pass_args[\"lottery_ticket\"][\"granularity\"],\n",
    "            \"method\": pass_args[\"lottery_ticket\"][\"method\"],\n",
    "            \"sparsity\": iteration_sparsity,\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"scope\": pass_args[\"lottery_ticket\"][\"scope\"],\n",
    "            \"granularity\": pass_args[\"lottery_ticket\"][\"granularity\"],\n",
    "            \"method\": pass_args[\"lottery_ticket\"][\"method\"],\n",
    "            \"sparsity\": iteration_sparsity,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "    original_w_b = {}\n",
    "\n",
    "    for node in mg.fx_graph.nodes:\n",
    "        if get_mase_op(node) in [\"linear\", \"conv2d\", \"conv1d\"]:\n",
    "            original_w_b[node.name] = {\n",
    "                \"weight\": mg.modules[node.target].weight,\n",
    "                \"bias\": mg.modules[node.target].bias,\n",
    "                \"meta_weight\": node.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"value\"],\n",
    "                \"meta_bias\": node.meta[\"mase\"].parameters[\"common\"][\"args\"][\"bias\"][\"value\"],\n",
    "            }\n",
    "            \n",
    "    for i in range(num_iterations):\n",
    "        mg, _ = prune_transform_pass(mg, prune_args)\n",
    "\n",
    "        train(**train_test_args)\n",
    "\n",
    "        # copy the weights from the original model to the pruned model\n",
    "        for node in mg.fx_graph.nodes:\n",
    "            if get_mase_op(node) in [\"linear\", \"conv2d\", \"conv1d\"]:\n",
    "                with torch.no_grad():\n",
    "                    mg.modules[node.target].weight.copy_(original_w_b[node.name]['weight'])\n",
    "                    # mg.modules[node.target].weight.copy_(original_w_b[node.name]['weight'])\n",
    "\n",
    "                    mg.modules[node.target].bias.copy_(original_w_b[node.name]['bias'])\n",
    "\n",
    "                    # update the mase metadata weights\n",
    "                    node.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"value\"] = original_w_b[node.name]['meta_weight']\n",
    "                    \n",
    "                    node.meta[\"mase\"].parameters[\"common\"][\"args\"][\"bias\"][\"value\"] = original_w_b[node.name]['meta_bias']\n",
    "\n",
    "    train(**train_test_args)\n",
    "\n",
    "    test(**train_test_args)\n",
    "\n",
    "    return mg, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "dataset_info = get_dataset_info(dataset_name)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "lt_train_params['data_module'] = data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 14.0 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | acc_val   | MulticlassAccuracy | 0     \n",
      "4 | acc_test  | MulticlassAccuracy | 0     \n",
      "5 | loss_val  | MeanMetric         | 0     \n",
      "6 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "56.118    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2237100e2b3e40c5b71b2b0a06139588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkt123/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/home/bkt123/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff137c26436141a18ca8d96a218e4dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a371a0047df343bea2cf77727fb0d781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 14.0 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | acc_val   | MulticlassAccuracy | 0     \n",
      "4 | acc_test  | MulticlassAccuracy | 0     \n",
      "5 | loss_val  | MeanMetric         | 0     \n",
      "6 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "56.118    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6f72f2462f43a5aaba13379381472f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53acd9c6f807473a95060dfe03839aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pass_args = {\n",
    "    \"lottery_ticket\": {\n",
    "        \"num_iterations\": 5,\n",
    "        \"scope\": \"global\",\n",
    "        \"granularity\": \"elementwise\",\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"sparsity\": 0.7\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"dataset\": \"cifar10\",\n",
    "        \"model\": \"vgg7\",\n",
    "        \"task\": \"cls\",\n",
    "        \"batch_size\": 256,\n",
    "        \"max_epochs\": 1,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "mg_lt, _ = prune_lottery_ticket(mg_lt, pass_args)\n",
    "\n",
    "lt_train_params[\"model\"] = mg_lt.model\n",
    "test(**lt_train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in mg_lt.fx_graph.nodes:\n",
    "    if get_mase_op(node) in ['linear', \"conv2d\", \"conv1d\"]:\n",
    "        print(node.name)\n",
    "        total_w = 0\n",
    "        pruned_w = 0\n",
    "        w = mg_lt.modules[node.target].weight\n",
    "        for s in w:\n",
    "            total_w += s.numel()\n",
    "            pruned_w += s.numel() - s[s.nonzero()].numel()\n",
    "\n",
    "        pruned_percent = pruned_w / total_w\n",
    "        print(f\"Pruned percent: {pruned_percent}\")\n",
    "        print(75*'-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
